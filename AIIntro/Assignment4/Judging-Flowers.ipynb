{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from sklearn library\n",
    "# separate the data and target\n",
    "# convert the returning to dataframes\n",
    "iris_data, iris_target = load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "COL_S_L = \"sepal length (cm)\"\n",
    "COL_S_W = \"sepal width (cm)\"\n",
    "COL_P_L = \"petal length (cm)\"\n",
    "COL_P_W = \"petal width (cm)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                 5.1               3.5                1.4               0.2\n",
       "1                 4.9               3.0                1.4               0.2\n",
       "2                 4.7               3.2                1.3               0.2\n",
       "3                 4.6               3.1                1.5               0.2\n",
       "4                 5.0               3.6                1.4               0.2\n",
       "5                 5.4               3.9                1.7               0.4\n",
       "6                 4.6               3.4                1.4               0.3\n",
       "7                 5.0               3.4                1.5               0.2\n",
       "8                 4.4               2.9                1.4               0.2\n",
       "9                 4.9               3.1                1.5               0.1\n",
       "10                5.4               3.7                1.5               0.2\n",
       "11                4.8               3.4                1.6               0.2\n",
       "12                4.8               3.0                1.4               0.1\n",
       "13                4.3               3.0                1.1               0.1\n",
       "14                5.8               4.0                1.2               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the first 15 rows of data\n",
    "iris_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 4 columns):\n",
      "sepal length (cm)    150 non-null float64\n",
      "sepal width (cm)     150 non-null float64\n",
      "petal length (cm)    150 non-null float64\n",
      "petal width (cm)     150 non-null float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 4.8 KB\n"
     ]
    }
   ],
   "source": [
    "# summary of the table information\n",
    "iris_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset  \n",
    "In this dataset, we have 4 features: the length of sepal, the width of speal, the length of petal and the width of petal. Our labels are the codes of different iris, which are stored in the `iris_target` series. In this dataset, we have 3 kind of iris, and we can use these 4 features to identify each iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features training size: 135\n",
      "features testing size: 15\n",
      "labels training size: 135\n",
      "labels testing size: 15\n"
     ]
    }
   ],
   "source": [
    "# 90% train and 10% test\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    iris_data, iris_target, test_size=0.1, random_state=53\n",
    ")\n",
    "print(f'features training size: {len(features_train)}')\n",
    "print(f'features testing size: {len(features_test)}')\n",
    "print(f'labels training size: {len(labels_train)}')\n",
    "print(f'labels testing size: {len(labels_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn to train a LogistiRegression model on the training set\n",
    "clf = LogisticRegression().fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[9.84881804e-01 1.51181819e-02 1.41387858e-08]]\n",
      "Actual label: 0\n"
     ]
    }
   ],
   "source": [
    "sample_index = 11\n",
    "sample_X = features_train.iloc[sample_index: sample_index+1, :]\n",
    "sample_y = labels_train.iloc[sample_index]\n",
    "# predict sample data\n",
    "probas = clf.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {probas}')\n",
    "print(f'Actual label: {sample_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result suggest that, for the 12th element in the training features, 98.5% it belongs to class 0, 1.5% it belongs to class 1 and 0.0000014% it belongs to class 0. This result fits to our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score` function returns the mean accuracy on the `features_test` and `labels_test`. The accuracy of 93.3% tells this model has a very good fit to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.38874638  0.98095147 -2.45856245 -1.05939201]\n",
      " [ 0.58105679 -0.37328934 -0.27315416 -0.8526411 ]\n",
      " [-0.19231041 -0.60766213  2.73171661  1.91203311]]\n",
      "[  9.33326993   2.19963106 -11.532901  ]\n"
     ]
    }
   ],
   "source": [
    "print(clf.coef_)\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use sklearn to train a SVC on training set\n",
    "svm_clf = svm.SVC(probability=True)\n",
    "svm_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[0.97801188 0.01343567 0.00855245]]\n",
      "Actual label: 0\n"
     ]
    }
   ],
   "source": [
    "svm_probas = svm_clf.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {svm_probas}')\n",
    "print(f'Actual label: {sample_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result suggest that, for the 12th element in the training features, 97.8% it belongs to class 0, 1.3% it belongs to class 1 and 0.88% it belongs to class 0. This result fits to our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score` function measures the mean accuracy on the given testing dataset.  \n",
    "The accuracy of 93.33% tells the model can perform a good classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Nural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "# train a MLP classifier on the training set\n",
    "MLP_clf = MLPClassifier(random_state=1).fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[0.95232573 0.04546975 0.00220452]]\n",
      "Actual label: 0\n"
     ]
    }
   ],
   "source": [
    "MLP_probas = MLP_clf.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {MLP_probas}')\n",
    "print(f'Actual label: {sample_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result suggest that, for the 12th element in the training features, 95.2% it belongs to class 0, 4.5% it belongs to class 1 and 0.22% it belongs to class 0. This result fits to our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `score` function tells us the mean accuracy on the given testing dataset.  \n",
    "Under the default 200 iterations and rectified linear unit function, the MLP classifer has the accracy of 86.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[9.98998468e-01 1.00153237e-03 2.61583050e-15]]\n",
      "0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "MLP_clf_600 = MLPClassifier(random_state=1, max_iter=600).fit(features_train, labels_train)\n",
    "MLP_probas_600 = MLP_clf_600.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {MLP_probas_600}')\n",
    "print(MLP_clf_600.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[9.99275346e-01 7.24653546e-04 5.88429271e-17]]\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "MLP_clf_700 = MLPClassifier(random_state=1, max_iter=700).fit(features_train, labels_train)\n",
    "MLP_probas_700 = MLP_clf_700.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {MLP_probas_700}')\n",
    "print(MLP_clf_700.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At around 700 iterations, the MLP classifer reaches to the optimal situation and the optimal accuracy is 93.33%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP_clf_logistic = MLPClassifier(random_state=1, activation=\"logistic\").fit(features_train, labels_train)\n",
    "MLP_clf_logistic.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[9.95398651e-01 4.60134902e-03 2.62218707e-12]]\n",
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "MLP_clf_logistic_1000 = MLPClassifier(random_state=1, max_iter=1000,activation=\"logistic\").fit(features_train, labels_train)\n",
    "MLP_probas_logistic_1000 = MLP_clf_logistic_1000.predict_proba(sample_X)\n",
    "print(f'Probabilites of each classes: {MLP_probas_logistic_1000}')\n",
    "print(MLP_clf_logistic_1000.score(features_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using logistic as activation function and run 1000 iteration, this MLP classifier reaches to the optimal situation and the accuracy is 93.33%.  \n",
    "This accuracy is the same as the classifier using relu as activation function, but this flassifier needs more iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a k-neighobrs classifier\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(features_train.values, labels_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilites of each classes: [[1. 0. 0.]]\n",
      "Actual label: 0\n"
     ]
    }
   ],
   "source": [
    "neigh_probas = neigh.predict_proba(sample_X.values)\n",
    "print(f'Probabilites of each classes: {neigh_probas}')\n",
    "print(f'Actual label: {sample_y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN classifier classifies the sample X as the class 0, which matches our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(features_test.values, labels_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the default 5 neighbors and let the `KNeighborsCalssifier` class decide the algorithm.  \n",
    "And the KNN model gives us the accuracy of 93.33%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Conclusions and takeaways  \n",
    "By looking at the scores of all these models, we can learn that for this setup, the best accuracy we can have is about 93.33%.    \n",
    "  \n",
    "The Neural Network does the best job in my opinion. When we configurate the neural network to reach the optimal situation, the neural network using logistic funciton as the activation function gives us the fittest prediction on the sample datapoint. It tells us the 12th datapoint in the training feature dataset is 99.54% should be class 0, while logistic regression says it is 98.49%, svm says it is 97.8%.  \n",
    "KNN works differently and it tells us the 12th datapoint is class 0.  \n",
    "But the neural network needs us to setup the settings to get the best result. If we don't, the default setup gives us an accuracy of 86.67%, which is way worse than other models.  \n",
    "Considering that the logistic regression just runs 100 iterations and has a similar result with the optimal neural network, which required 1000 iterations, logistic regression has the best overall performance on regonizing iris.  \n",
    "  \n",
    "The thing surprise me is that the neural network requires much more configurations and resources. It has different activation functions and it needs a large numebrs of iterations to acheve the optimal model. These configurations can really improve the performance of a neural network, but on regonizing iris, the improvment is limited when comparing with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
